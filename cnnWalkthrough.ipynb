{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "### let's get some of the necessary libraries in here\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms  \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transform \n",
    "from torchvision.transforms import ToTensor, Normalize, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = \"/Users/John/Documents/allProjects/intelData/seg_train/seg_train\" \n",
    "dataTest = \"/Users/John/Documents/allProjects/intelData/seg_test/seg_test\"\n",
    "dataPred = \"/Users/John/Documents/allProjects/intelData/seg_pred/seg_pred\"\n",
    "\n",
    "## the folders in the training data give us an easy list of labels\n",
    "labels = os.listdir(dataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forest', 'buildings', 'glacier', 'street', 'mountain', 'sea']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing image tensors\n",
    "This process is important because it helps minimize the impact of image brightness and contrast amongst all images. Unless the images are taken school yearbook style - same location, lighting, camera, etc. - then there will inevitably be some differences among the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTemp = ImageFolder(dataTrain, transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.RandomCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "]))\n",
    "trainDL = DataLoader(trainTemp, 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## do this to extract a single image\n",
    "for (image, label) in list(enumerate(trainDL))[:1]:\n",
    "    print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dimensions are [64, 3, 64, 64] so we only need the means and sds for positions 0, 2, 3. We can discared 1 via the dim argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMeanSD(DL):\n",
    "    \"\"\"\n",
    "    This function will calculate the mean and sum of squared mean for the data in a\n",
    "    DataLoader. I adjusted it specifically to this dataset via the dim argument, skipping\n",
    "    index 1 because that was not the data required. This function may require other adjustments\n",
    "    for other datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    ## initialize the three variables as zero\n",
    "    runningSum, sumSquared, batches = 0,0,0\n",
    "    \n",
    "    ## extract the data from the DataLoader and calculate the sums and sum squared\n",
    "    for data, label in DL:\n",
    "        runningSum += torch.mean(data, dim = ([0,2,3]))\n",
    "        sumSquared += torch.mean(data**2, dim = ([0,2,3]))\n",
    "        batches += 1\n",
    "\n",
    "    ## simple calcs of     \n",
    "    mean = runningSum/batches\n",
    "    std = (sumSquared/batches - mean**2)**0.5\n",
    "    return mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, sd  = calculateMeanSD(trainDL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our means and standard deviations are below. This will inform the normalize step of any transformations within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting training data\n",
    "The key here is to add in some randomness so that the CNN detects changes in the image. CNNs are shift invariant, meaning they will detect key details regardless of the position, yet they would struggle if I flipped, cropped, stretched, etc. the images. As such, we'll add in some of that randomness to help the CNN perform better on images with unique traits.\n",
    "\n",
    "To do this I, resize, add a random crop, a random color jitter, and a random horizontal flip. This is quite a bit of randomness, which should introduce the CNN to a lot of different varieties. I'll likely want to crank the epochs to make sure the CNN \"sees\" everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we'll do some augmentation on the training data\n",
    "trainTransform = transform.Compose([\n",
    "    transform.Resize((64,64)),\n",
    "    transform.RandomCrop((64,64)),\n",
    "    transforms.ColorJitter(0.3,0.4,0.4,0.2),\n",
    "    transform.RandomHorizontalFlip(), ## default is p = 0.5\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize((mean[0],mean[1],mean[2]), (sd[0], sd[1], sd[2]))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not augment test data because we are evaluating the model's ability to correctly identify the images as opposed to preparing it to identify key attributes anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTransform = transform.Compose([\n",
    "    transform.Resize((64,64)),\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize((mean[0],mean[1],mean[2]), (sd[0], sd[1], sd[2]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageFolder(dataTrain, transform = trainTransform)\n",
    "test = ImageFolder(dataTest, transform = testTransform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedA = 33\n",
    "torch.manual_seed(seedA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11227 2807\n",
      "11227 2807\n"
     ]
    }
   ],
   "source": [
    "## set the sizes and train test split the data\n",
    "train_size = int(0.8*len(train))\n",
    "val_size = int(len(train) - train_size)\n",
    "print(train_size, val_size)\n",
    "\n",
    "trainData, valData = random_split(train, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters and the device. We'd like to use my gpu if it is available. If it is not, we can use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 64\n",
    "classes = len(labels)\n",
    "learning_rate = 0.001\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the dataloaders\n",
    "train_dl = DataLoader(trainData, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "valid_dl = DataLoader(valData, batch_size*2, num_workers=2, pin_memory=True)\n",
    "test_dl = DataLoader(test, batch_size*2, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definine the Neural Network\n",
    "\n",
    "I'm going to do one bespoke CNN and one pretrained model for comparison purposes. To build a bespoke NN with pytorch we need to define the class with nn.Module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BespokeCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an extremely basic CNN, I doubt it will perform as well as the\n",
    "    pretrained models.\n",
    "    \"\"\"\n",
    "\t## Decide upon some layers within the network\n",
    "    def __init__(self, classes):\n",
    "        super(BespokeCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        ## fully connected layers with Softmax as the activation function\n",
    "        ## Using softmax because this is a multi-classification problem\n",
    "        self.fc1 = nn.Linear(12544, 128)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        self.fc2 = nn.Linear(128, classes)\n",
    "    \n",
    "    ## defines how the data moves across the layers\n",
    "    def forward(self,x):\n",
    "        forw = self.conv1(x)\n",
    "        forw = self.max_pool1(forw)\n",
    "        \n",
    "        forw = self.conv2(forw)\n",
    "        forw = self.max_pool2(forw)\n",
    "                \n",
    "        forw = forw.reshape(forw.size(0), -1)\n",
    "        \n",
    "        forw = self.fc1(forw)\n",
    "        forw = self.softmax(forw)\n",
    "        forw = self.fc2(forw)\n",
    "\n",
    "        return forw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the model from the class we built\n",
    "model = BespokeCNN(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, epochs, train_dl, valid_dl, learning_rate):\n",
    "    \"\"\"\n",
    "    This function will train the provided model printing the accuracy for the validation data.\n",
    "    It takes the model, number of epochs, and two data loaders and then runs the model through the \n",
    "    training process. If we want to change the loss function or optimizer, we can in the code below\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.001, momentum = 0.9)  \n",
    "    #total_step = len(train_dl)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        #Load in the data in batches using the train_loader object\n",
    "        for i, (images, labels) in enumerate(train_dl):  \n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        ## let's monitor the prediction accuracy on the validation data for each epoch\n",
    "        ## this may indicate a good stopping point for the particular nertal network\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in valid_dl:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model2(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            print('Accuracy on {} validation images: {:.4f} %'.format(val_size, 100 * correct / total))\n",
    "\n",
    "        end = time.time()\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}, Time in Seconds: {:.4f}'.format(epoch+1, num_epochs, loss.item(), end - start))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 17.9195 %\n",
      "Epoch [1/50], Loss: 1.8011, Time in Seconds: 23.7436\n",
      "Accuracy on 2807 validation images: 17.9195 %\n",
      "Epoch [1/50], Loss: 1.8011, Time in Seconds: 23.7436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 18.5607 %\n",
      "Epoch [2/50], Loss: 1.7996, Time in Seconds: 23.3876\n",
      "Accuracy on 2807 validation images: 18.5607 %\n",
      "Epoch [2/50], Loss: 1.7996, Time in Seconds: 23.3876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 18.2401 %\n",
      "Epoch [3/50], Loss: 1.7967, Time in Seconds: 23.1383\n",
      "Accuracy on 2807 validation images: 18.2401 %\n",
      "Epoch [3/50], Loss: 1.7967, Time in Seconds: 23.1383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 18.0264 %\n",
      "Epoch [4/50], Loss: 1.7942, Time in Seconds: 23.6105\n",
      "Accuracy on 2807 validation images: 18.0264 %\n",
      "Epoch [4/50], Loss: 1.7942, Time in Seconds: 23.6105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 17.8839 %\n",
      "Epoch [5/50], Loss: 1.7932, Time in Seconds: 23.2763\n",
      "Accuracy on 2807 validation images: 17.8839 %\n",
      "Epoch [5/50], Loss: 1.7932, Time in Seconds: 23.2763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 18.3826 %\n",
      "Epoch [6/50], Loss: 1.7780, Time in Seconds: 23.8703\n",
      "Accuracy on 2807 validation images: 18.3826 %\n",
      "Epoch [6/50], Loss: 1.7780, Time in Seconds: 23.8703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 18.3114 %\n",
      "Epoch [7/50], Loss: 1.7829, Time in Seconds: 23.7921\n",
      "Accuracy on 2807 validation images: 18.3114 %\n",
      "Epoch [7/50], Loss: 1.7829, Time in Seconds: 23.7921\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 18.5251 %\n",
      "Epoch [8/50], Loss: 1.7608, Time in Seconds: 23.5456\n",
      "Accuracy on 2807 validation images: 18.5251 %\n",
      "Epoch [8/50], Loss: 1.7608, Time in Seconds: 23.5456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 17.9195 %\n",
      "Epoch [9/50], Loss: 1.7262, Time in Seconds: 23.5719\n",
      "Accuracy on 2807 validation images: 17.9195 %\n",
      "Epoch [9/50], Loss: 1.7262, Time in Seconds: 23.5719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 18.3114 %\n",
      "Epoch [10/50], Loss: 1.7280, Time in Seconds: 23.5538\n",
      "Accuracy on 2807 validation images: 18.3114 %\n",
      "Epoch [10/50], Loss: 1.7280, Time in Seconds: 23.5538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 18.0264 %\n",
      "Epoch [11/50], Loss: 1.6231, Time in Seconds: 23.5811\n",
      "Accuracy on 2807 validation images: 18.0264 %\n",
      "Epoch [11/50], Loss: 1.6231, Time in Seconds: 23.5811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 18.5607 %\n",
      "Epoch [12/50], Loss: 1.5918, Time in Seconds: 23.6162\n",
      "Accuracy on 2807 validation images: 18.5607 %\n",
      "Epoch [12/50], Loss: 1.5918, Time in Seconds: 23.6162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 18.2757 %\n",
      "Epoch [13/50], Loss: 1.5591, Time in Seconds: 23.4580\n",
      "Accuracy on 2807 validation images: 18.2757 %\n",
      "Epoch [13/50], Loss: 1.5591, Time in Seconds: 23.4580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 17.9551 %\n",
      "Epoch [14/50], Loss: 1.6567, Time in Seconds: 23.7193\n",
      "Accuracy on 2807 validation images: 17.9551 %\n",
      "Epoch [14/50], Loss: 1.6567, Time in Seconds: 23.7193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 17.7414 %\n",
      "Epoch [15/50], Loss: 1.6063, Time in Seconds: 23.4093\n",
      "Accuracy on 2807 validation images: 17.7414 %\n",
      "Epoch [15/50], Loss: 1.6063, Time in Seconds: 23.4093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 17.7057 %\n",
      "Epoch [16/50], Loss: 1.4723, Time in Seconds: 23.9243\n",
      "Accuracy on 2807 validation images: 17.7057 %\n",
      "Epoch [16/50], Loss: 1.4723, Time in Seconds: 23.9243\n"
     ]
    }
   ],
   "source": [
    "model = trainModel(model, epochs, train_dl, valid_dl, learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy improved as the model saw more data, which is usually the case; however it's poor. My guess is that the BespokeCNN is quite shallow. I'm going to add some layers to make it a deeper CNN and retest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an extremely basic CNN, I doubt it will perform as well as the\n",
    "    pretrained models.\n",
    "    \"\"\"\n",
    "\t## Decide upon some layers within the network\n",
    "    def __init__(self, classes):\n",
    "        super(BespokeCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.conv1 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        ## fully connected layers with Softmax as the activation function\n",
    "        ## Using softmax because this is a multi-classification problem\n",
    "        self.fc1 = nn.Linear(12544, 128)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        self.fc2 = nn.Linear(128, classes)\n",
    "    \n",
    "    ## defines how the data moves across the layers\n",
    "    def forward(self,x):\n",
    "        forw = self.conv1(x)\n",
    "        forw = self.max_pool1(forw)\n",
    "        \n",
    "        forw = self.conv2(forw)\n",
    "        forw = self.max_pool2(forw)\n",
    "                \n",
    "        forw = forw.reshape(forw.size(0), -1)\n",
    "        \n",
    "        forw = self.fc1(forw)\n",
    "        forw = self.softmax(forw)\n",
    "        forw = self.fc2(forw)\n",
    "\n",
    "        return forw"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
