{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### let's get some of the necessary libraries in here\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms  \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transform \n",
    "from torchvision.transforms import ToTensor, Normalize, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = \"/Users/John/Documents/allProjects/intelData/seg_train/seg_train\" \n",
    "dataTest = \"/Users/John/Documents/allProjects/intelData/seg_test/seg_test\"\n",
    "dataPred = \"/Users/John/Documents/allProjects/intelData/seg_pred/seg_pred\"\n",
    "\n",
    "## the folders in the training data give us an easy list of labels\n",
    "labels = os.listdir(dataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forest', 'buildings', 'glacier', 'street', 'mountain', 'sea']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing image tensors\n",
    "This process is important because it helps minimize the impact of image brightness and contrast amongst all images. Unless the images are taken school yearbook style - same location, lighting, camera, etc. - then there will inevitably be some differences among the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTemp = ImageFolder(dataTrain, transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.RandomCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "]))\n",
    "trainDL = DataLoader(trainTemp, 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## do this to extract a single image\n",
    "for (image, label) in list(enumerate(trainDL))[:1]:\n",
    "    print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dimensions are [64, 3, 64, 64] so we only need the means and sds for positions 0, 2, 3. We can discared 1 via the dim argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMeanSD(DL):\n",
    "    \"\"\"\n",
    "    This function will calculate the mean and sum of squared mean for the data in a\n",
    "    DataLoader. I adjusted it specifically to this dataset via the dim argument, skipping\n",
    "    index 1 because that was not the data required. This function may require other adjustments\n",
    "    for other datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    ## initialize the three variables as zero\n",
    "    runningSum, sumSquared, batches = 0,0,0\n",
    "    \n",
    "    ## extract the data from the DataLoader and calculate the sums and sum squared\n",
    "    for data, label in DL:\n",
    "        runningSum += torch.mean(data, dim = ([0,2,3]))\n",
    "        sumSquared += torch.mean(data**2, dim = ([0,2,3]))\n",
    "        batches += 1\n",
    "\n",
    "    ## simple calcs of     \n",
    "    mean = runningSum/batches\n",
    "    std = (sumSquared/batches - mean**2)**0.5\n",
    "    return mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, sd  = calculateMeanSD(trainDL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our means and standard deviations are below. This will inform the normalize step of any transformations within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting training data\n",
    "The key here is to add in some randomness so that the CNN detects changes in the image. CNNs are shift invariant, meaning they will detect key details regardless of the position, yet they would struggle if I flipped, cropped, stretched, etc. the images. As such, we'll add in some of that randomness to help the CNN perform better on images with unique traits.\n",
    "\n",
    "To do this I, resize, add a random crop, a random color jitter, and a random horizontal flip. This is quite a bit of randomness, which should introduce the CNN to a lot of different varieties. I'll likely want to crank the epochs to make sure the CNN \"sees\" everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we'll do some augmentation on the training data\n",
    "trainTransform = transform.Compose([\n",
    "    transform.Resize((64,64)),\n",
    "    transform.RandomCrop((64,64)),\n",
    "    transforms.ColorJitter(0.3,0.4,0.4,0.2),\n",
    "    transform.RandomHorizontalFlip(), ## default is p = 0.5\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize((mean[0],mean[1],mean[2]), (sd[0], sd[1], sd[2]))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not augment test data because we are evaluating the model's ability to correctly identify the images as opposed to preparing it to identify key attributes anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTransform = transform.Compose([\n",
    "    transform.Resize((64,64)),\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize((mean[0],mean[1],mean[2]), (sd[0], sd[1], sd[2]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageFolder(dataTrain, transform = trainTransform)\n",
    "test = ImageFolder(dataTest, transform = testTransform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedA = 33\n",
    "torch.manual_seed(seedA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11227 2807\n",
      "11227 2807\n"
     ]
    }
   ],
   "source": [
    "## set the sizes and train test split the data\n",
    "train_size = int(0.8*len(train))\n",
    "val_size = int(len(train) - train_size)\n",
    "print(train_size, val_size)\n",
    "\n",
    "trainData, valData = random_split(train, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters and the device. We'd like to use my gpu if it is available. If it is not, we can use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 64\n",
    "classes = len(labels)\n",
    "learning_rate = 0.001\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the dataloaders\n",
    "train_dl = DataLoader(trainData, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "valid_dl = DataLoader(valData, batch_size*2, num_workers=2, pin_memory=True)\n",
    "test_dl = DataLoader(test, batch_size*2, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definine the Neural Network\n",
    "\n",
    "I'm going to do one bespoke CNN and one pretrained model for comparison purposes. To build a bespoke NN with pytorch we need to define the class with nn.Module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BespokeCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an extremely basic CNN, I doubt it will perform as well as the\n",
    "    pretrained models.\n",
    "    \"\"\"\n",
    "\t## Decide upon some layers within the network\n",
    "    def __init__(self, classes):\n",
    "        super(BespokeCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        ## fully connected layers with Softmax as the activation function\n",
    "        ## Using softmax because this is a multi-classification problem\n",
    "        self.fc1 = nn.Linear(12544, 128)\n",
    "        self.fc2 = nn.Linear(128, classes)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "    \n",
    "    ## defines how the data moves across the layers\n",
    "    def forward(self,x):\n",
    "        forw = self.conv1(x)\n",
    "        forw = self.max_pool1(forw)\n",
    "        \n",
    "        forw = self.conv2(forw)\n",
    "        forw = self.max_pool2(forw)\n",
    "                \n",
    "        forw = forw.reshape(forw.size(0), -1)\n",
    "        \n",
    "        forw = self.fc1(forw)\n",
    "        forw = self.fc2(forw)\n",
    "        forw = self.softmax(forw)\n",
    "        \n",
    "\n",
    "        return forw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the model from the class we built\n",
    "model = BespokeCNN(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, epochs, train_dl, valid_dl, learning_rate):\n",
    "    \"\"\"\n",
    "    This function will train the provided model printing the accuracy for the validation data.\n",
    "    It takes the model, number of epochs, and two data loaders and then runs the model through the \n",
    "    training process. If we want to change the loss function or optimizer, we can in the code below\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.001, momentum = 0.9)  \n",
    "    #total_step = len(train_dl)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        #Load in the data in batches using the train_loader object\n",
    "        for i, (images, labels) in enumerate(train_dl):  \n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        ## let's monitor the prediction accuracy on the validation data for each epoch\n",
    "        ## this may indicate a good stopping point for the particular neural network\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in valid_dl:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            print('Accuracy on {} validation images: {:.4f} %'.format(val_size, 100 * correct / total))\n",
    "\n",
    "        end = time.time()\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}, Time in Seconds: {:.4f}'.format(epoch+1, epochs, loss.item(), end - start))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 52.7253 %\n",
      "Epoch [1/25], Loss: 1.5926, Time in Seconds: 24.4088\n",
      "Accuracy on 2807 validation images: 52.7253 %\n",
      "Epoch [1/25], Loss: 1.5926, Time in Seconds: 24.4088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 54.9341 %\n",
      "Epoch [2/25], Loss: 1.5307, Time in Seconds: 23.9522\n",
      "Accuracy on 2807 validation images: 54.9341 %\n",
      "Epoch [2/25], Loss: 1.5307, Time in Seconds: 23.9522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 54.9341 %\n",
      "Epoch [3/25], Loss: 1.5617, Time in Seconds: 23.7382\n",
      "Accuracy on 2807 validation images: 54.9341 %\n",
      "Epoch [3/25], Loss: 1.5617, Time in Seconds: 23.7382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 55.8603 %\n",
      "Epoch [4/25], Loss: 1.4687, Time in Seconds: 23.8442\n",
      "Accuracy on 2807 validation images: 55.8603 %\n",
      "Epoch [4/25], Loss: 1.4687, Time in Seconds: 23.8442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 56.5372 %\n",
      "Epoch [5/25], Loss: 1.3823, Time in Seconds: 23.3743\n",
      "Accuracy on 2807 validation images: 56.5372 %\n",
      "Epoch [5/25], Loss: 1.3823, Time in Seconds: 23.3743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 56.1454 %\n",
      "Epoch [6/25], Loss: 1.3027, Time in Seconds: 23.8263\n",
      "Accuracy on 2807 validation images: 56.1454 %\n",
      "Epoch [6/25], Loss: 1.3027, Time in Seconds: 23.8263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 57.3922 %\n",
      "Epoch [7/25], Loss: 1.4306, Time in Seconds: 23.6338\n",
      "Accuracy on 2807 validation images: 57.3922 %\n",
      "Epoch [7/25], Loss: 1.4306, Time in Seconds: 23.6338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 59.1022 %\n",
      "Epoch [8/25], Loss: 1.5030, Time in Seconds: 23.8514\n",
      "Accuracy on 2807 validation images: 59.1022 %\n",
      "Epoch [8/25], Loss: 1.5030, Time in Seconds: 23.8514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 58.4966 %\n",
      "Epoch [9/25], Loss: 1.2865, Time in Seconds: 23.8525\n",
      "Accuracy on 2807 validation images: 58.4966 %\n",
      "Epoch [9/25], Loss: 1.2865, Time in Seconds: 23.8525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 58.3185 %\n",
      "Epoch [10/25], Loss: 1.4515, Time in Seconds: 23.5675\n",
      "Accuracy on 2807 validation images: 58.3185 %\n",
      "Epoch [10/25], Loss: 1.4515, Time in Seconds: 23.5675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 60.5629 %\n",
      "Epoch [11/25], Loss: 1.2663, Time in Seconds: 23.7524\n",
      "Accuracy on 2807 validation images: 60.5629 %\n",
      "Epoch [11/25], Loss: 1.2663, Time in Seconds: 23.7524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 59.6366 %\n",
      "Epoch [12/25], Loss: 1.4705, Time in Seconds: 23.5645\n",
      "Accuracy on 2807 validation images: 59.6366 %\n",
      "Epoch [12/25], Loss: 1.4705, Time in Seconds: 23.5645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 60.7054 %\n",
      "Epoch [13/25], Loss: 1.4525, Time in Seconds: 23.9887\n",
      "Accuracy on 2807 validation images: 60.7054 %\n",
      "Epoch [13/25], Loss: 1.4525, Time in Seconds: 23.9887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 62.0235 %\n",
      "Epoch [14/25], Loss: 1.4016, Time in Seconds: 23.7018\n",
      "Accuracy on 2807 validation images: 62.0235 %\n",
      "Epoch [14/25], Loss: 1.4016, Time in Seconds: 23.7018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 60.4560 %\n",
      "Epoch [15/25], Loss: 1.4029, Time in Seconds: 23.4981\n",
      "Accuracy on 2807 validation images: 60.4560 %\n",
      "Epoch [15/25], Loss: 1.4029, Time in Seconds: 23.4981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 62.5579 %\n",
      "Epoch [16/25], Loss: 1.4704, Time in Seconds: 23.9585\n",
      "Accuracy on 2807 validation images: 62.5579 %\n",
      "Epoch [16/25], Loss: 1.4704, Time in Seconds: 23.9585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 63.2348 %\n",
      "Epoch [17/25], Loss: 1.3993, Time in Seconds: 23.6773\n",
      "Accuracy on 2807 validation images: 63.2348 %\n",
      "Epoch [17/25], Loss: 1.3993, Time in Seconds: 23.6773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 62.7360 %\n",
      "Epoch [18/25], Loss: 1.4288, Time in Seconds: 23.7531\n",
      "Accuracy on 2807 validation images: 62.7360 %\n",
      "Epoch [18/25], Loss: 1.4288, Time in Seconds: 23.7531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 64.8735 %\n",
      "Epoch [19/25], Loss: 1.3159, Time in Seconds: 23.6075\n",
      "Accuracy on 2807 validation images: 64.8735 %\n",
      "Epoch [19/25], Loss: 1.3159, Time in Seconds: 23.6075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 63.3060 %\n",
      "Epoch [20/25], Loss: 1.3714, Time in Seconds: 23.4025\n",
      "Accuracy on 2807 validation images: 63.3060 %\n",
      "Epoch [20/25], Loss: 1.3714, Time in Seconds: 23.4025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 64.0542 %\n",
      "Epoch [21/25], Loss: 1.4479, Time in Seconds: 24.8049\n",
      "Accuracy on 2807 validation images: 64.0542 %\n",
      "Epoch [21/25], Loss: 1.4479, Time in Seconds: 24.8049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 63.1635 %\n",
      "Epoch [22/25], Loss: 1.3894, Time in Seconds: 24.2381\n",
      "Accuracy on 2807 validation images: 63.1635 %\n",
      "Epoch [22/25], Loss: 1.3894, Time in Seconds: 24.2381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 64.7310 %\n",
      "Epoch [23/25], Loss: 1.4283, Time in Seconds: 23.6714\n",
      "Accuracy on 2807 validation images: 64.7310 %\n",
      "Epoch [23/25], Loss: 1.4283, Time in Seconds: 23.6714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 63.4129 %\n",
      "Epoch [24/25], Loss: 1.3788, Time in Seconds: 24.1553\n",
      "Accuracy on 2807 validation images: 63.4129 %\n",
      "Epoch [24/25], Loss: 1.3788, Time in Seconds: 24.1553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 65.3723 %\n",
      "Epoch [25/25], Loss: 1.3588, Time in Seconds: 24.7434\n",
      "Accuracy on 2807 validation images: 65.3723 %\n",
      "Epoch [25/25], Loss: 1.3588, Time in Seconds: 24.7434\n"
     ]
    }
   ],
   "source": [
    "modelOut = trainModel(model, 25, train_dl, valid_dl, learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 25 epochs the loss function is headed downwards and the prediction accuracy is increasing. This suggests more training cycles could be helpful. I'm going to re-run with 50 to see what I get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 42.7146 %\n",
      "Epoch [1/50], Loss: 1.6391, Time in Seconds: 25.3216\n",
      "Accuracy on 2807 validation images: 42.7146 %\n",
      "Epoch [1/50], Loss: 1.6391, Time in Seconds: 25.3216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 48.8422 %\n",
      "Epoch [2/50], Loss: 1.5251, Time in Seconds: 24.1999\n",
      "Accuracy on 2807 validation images: 48.8422 %\n",
      "Epoch [2/50], Loss: 1.5251, Time in Seconds: 24.1999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 52.6185 %\n",
      "Epoch [3/50], Loss: 1.3587, Time in Seconds: 23.9948\n",
      "Accuracy on 2807 validation images: 52.6185 %\n",
      "Epoch [3/50], Loss: 1.3587, Time in Seconds: 23.9948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 52.9035 %\n",
      "Epoch [4/50], Loss: 1.6053, Time in Seconds: 24.3778\n",
      "Accuracy on 2807 validation images: 52.9035 %\n",
      "Epoch [4/50], Loss: 1.6053, Time in Seconds: 24.3778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 54.0791 %\n",
      "Epoch [5/50], Loss: 1.5078, Time in Seconds: 23.8178\n",
      "Accuracy on 2807 validation images: 54.0791 %\n",
      "Epoch [5/50], Loss: 1.5078, Time in Seconds: 23.8178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 54.9341 %\n",
      "Epoch [6/50], Loss: 1.5962, Time in Seconds: 23.9399\n",
      "Accuracy on 2807 validation images: 54.9341 %\n",
      "Epoch [6/50], Loss: 1.5962, Time in Seconds: 23.9399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 54.6847 %\n",
      "Epoch [7/50], Loss: 1.5428, Time in Seconds: 23.8549\n",
      "Accuracy on 2807 validation images: 54.6847 %\n",
      "Epoch [7/50], Loss: 1.5428, Time in Seconds: 23.8549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 56.6797 %\n",
      "Epoch [8/50], Loss: 1.5144, Time in Seconds: 23.5247\n",
      "Accuracy on 2807 validation images: 56.6797 %\n",
      "Epoch [8/50], Loss: 1.5144, Time in Seconds: 23.5247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 56.1454 %\n",
      "Epoch [9/50], Loss: 1.5776, Time in Seconds: 23.7885\n",
      "Accuracy on 2807 validation images: 56.1454 %\n",
      "Epoch [9/50], Loss: 1.5776, Time in Seconds: 23.7885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 56.7510 %\n",
      "Epoch [10/50], Loss: 1.3387, Time in Seconds: 24.4587\n",
      "Accuracy on 2807 validation images: 56.7510 %\n",
      "Epoch [10/50], Loss: 1.3387, Time in Seconds: 24.4587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 57.8197 %\n",
      "Epoch [11/50], Loss: 1.3349, Time in Seconds: 24.1277\n",
      "Accuracy on 2807 validation images: 57.8197 %\n",
      "Epoch [11/50], Loss: 1.3349, Time in Seconds: 24.1277\n"
     ]
    }
   ],
   "source": [
    "modelOut = trainModel(model, 50, train_dl, valid_dl, learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model stinks, as expected. The network is very shallow and skips some key steps. It is now time to move on to work that is reputable and researched.\n",
    "\n",
    "Unless you are conducting research in the neural network realm to improve performance, building a bespoke model probably isn't a great idea. Sticking to tried and true methods will almost always be the best way to go. Due to the No Free Lunch Theorem, it makes the most sense to try many models, some of which are built into pytorch.\n",
    "\n",
    "Next, I'm going to implement ResNet9. The literature on the model showed a 92% accuracy on the dataset I'm using - that's pretty good. There are some complex elements to it, so there will be quite a bit of handwritten coding to implement this, which is fine. The idea is that this code will work for any image dataset that I choose even though I'm using the one that this model was initially trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet9\n",
    "\n",
    "Now that our exceptionally poor bespoke CNN has run, we are going to mix things up and go with a well-researched approach: ResNet9. I chose this because the researchers applied this model to the dataset I'm working with, as such, I'm replicating the paper itself. It's important to note that I need to implement the training and validation procedure because they dynamically adjust the learning rate, as well as the architecture of the neural net itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This block will define a class that allows me to train and validate the images as we make predictions through our epochs\n",
    "\n",
    "class ImageNet(nn.Module):\n",
    "    def training(self, batch):\n",
    "        \"\"\"\n",
    "        Takes the images and self, trains the model, calculates the loss and returns it\n",
    "        \"\"\"\n",
    "        \n",
    "        imgs, labs = batch\n",
    "        output = self(imgs)\n",
    "        loss = nn.CrossEntropyLoss(output, labs)\n",
    "        return loss\n",
    "\n",
    "    def validation(self, batch):\n",
    "        \"\"\"\n",
    "        Similar to training except we all calculate the accuracy.\n",
    "        \"\"\"\n",
    "        imgs, labs = batch\n",
    "        output = self(imgs)\n",
    "        loss = nn.CrossEntropyLoss(output,labs)\n",
    "        ### position 1 tells us the index for the largest predicted value\n",
    "        preds = torch.max(outputs, dim = 1)\n",
    "        acc = torch.tensor(torch.sum(preds[1] == labels).item() / len(preds))\n",
    "    return loss, acc\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
