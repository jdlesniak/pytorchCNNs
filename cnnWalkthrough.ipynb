{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### let's get some of the necessary libraries in here\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms  \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transform \n",
    "from torchvision.transforms import ToTensor, Normalize, Resize\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataTrain = \"/Users/John/Documents/allProjects/intelData/seg_train/seg_train\" \n",
    "dataTest = \"/Users/John/Documents/allProjects/intelData/seg_test/seg_test\"\n",
    "dataPred = \"/Users/John/Documents/allProjects/intelData/seg_pred/seg_pred\"\n",
    "\n",
    "## the folders in the training data give us an easy list of labels\n",
    "labels = os.listdir(dataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forest', 'buildings', 'glacier', 'street', 'mountain', 'sea']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing image tensors\n",
    "This process is important because it helps minimize the impact of image brightness and contrast amongst all images. Unless the images are taken school yearbook style - same location, lighting, camera, etc. - then there will inevitably be some differences among the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainTemp = ImageFolder(dataTrain, transform = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    transforms.RandomCrop(64),\n",
    "    transforms.ToTensor(),\n",
    "]))\n",
    "trainDL = DataLoader(trainTemp, 64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "## do this to extract a single image\n",
    "for (image, label) in list(enumerate(trainDL))[:1]:\n",
    "    print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dimensions are [64, 3, 64, 64] so we only need the means and sds for positions 0, 2, 3. We can discared 1 via the dim argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 64, 64])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMeanSD(DL):\n",
    "    \"\"\"\n",
    "    This function will calculate the mean and sum of squared mean for the data in a\n",
    "    DataLoader. I adjusted it specifically to this dataset via the dim argument, skipping\n",
    "    index 1 because that was not the data required. This function may require other adjustments\n",
    "    for other datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    ## initialize the three variables as zero\n",
    "    runningSum, sumSquared, batches = 0,0,0\n",
    "    \n",
    "    ## extract the data from the DataLoader and calculate the sums and sum squared\n",
    "    for data, label in DL:\n",
    "        runningSum += torch.mean(data, dim = ([0,2,3]))\n",
    "        sumSquared += torch.mean(data**2, dim = ([0,2,3]))\n",
    "        batches += 1\n",
    "\n",
    "    ## simple calcs of     \n",
    "    mean = runningSum/batches\n",
    "    std = (sumSquared/batches - mean**2)**0.5\n",
    "    return mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, sd  = calculateMeanSD(trainDL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our means and standard deviations are below. This will inform the normalize step of any transformations within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting training data\n",
    "The key here is to add in some randomness so that the CNN detects changes in the image. CNNs are shift invariant, meaning they will detect key details regardless of the position, yet they would struggle if I flipped, cropped, stretched, etc. the images. As such, we'll add in some of that randomness to help the CNN perform better on images with unique traits.\n",
    "\n",
    "To do this I, resize, add a random crop, a random color jitter, and a random horizontal flip. This is quite a bit of randomness, which should introduce the CNN to a lot of different varieties. I'll likely want to crank the epochs to make sure the CNN \"sees\" everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we'll do some augmentation on the training data\n",
    "trainTransform = transform.Compose([\n",
    "    transform.Resize((64,64)),\n",
    "    transform.RandomCrop((64,64)),\n",
    "    transforms.ColorJitter(0.3,0.4,0.4,0.2),\n",
    "    transform.RandomHorizontalFlip(), ## default is p = 0.5\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize((mean[0],mean[1],mean[2]), (sd[0], sd[1], sd[2]))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not augment test data because we are evaluating the model's ability to correctly identify the images as opposed to preparing it to identify key attributes anywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testTransform = transform.Compose([\n",
    "    transform.Resize((64,64)),\n",
    "    transform.ToTensor(),\n",
    "    transform.Normalize((mean[0],mean[1],mean[2]), (sd[0], sd[1], sd[2]))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ImageFolder(dataTrain, transform = trainTransform)\n",
    "test = ImageFolder(dataTest, transform = testTransform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedA = 33\n",
    "torch.manual_seed(seedA);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11227 2807\n",
      "11227 2807\n"
     ]
    }
   ],
   "source": [
    "## set the sizes and train test split the data\n",
    "train_size = int(0.8*len(train))\n",
    "val_size = int(len(train) - train_size)\n",
    "print(train_size, val_size)\n",
    "\n",
    "trainData, valData = random_split(train, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters and the device. We'd like to use my gpu if it is available. If it is not, we can use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 64\n",
    "classes = len(labels)\n",
    "learning_rate = 0.001\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the dataloaders\n",
    "train_dl = DataLoader(trainData, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "valid_dl = DataLoader(valData, batch_size*2, num_workers=2, pin_memory=True)\n",
    "test_dl = DataLoader(test, batch_size*2, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definine the Neural Network\n",
    "\n",
    "I'm going to do one bespoke CNN and one pretrained model for comparison purposes. To build a bespoke NN with pytorch we need to define the class with nn.Module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BespokeCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    This is an extremely basic CNN, I doubt it will perform as well as the\n",
    "    pretrained models.\n",
    "    \"\"\"\n",
    "\t## Decide upon some layers within the network\n",
    "    def __init__(self, classes):\n",
    "        super(BespokeCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.max_pool1 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "\n",
    "        ## fully connected layers with Softmax as the activation function\n",
    "        ## Using softmax because this is a multi-classification problem\n",
    "        self.fc1 = nn.Linear(12544, 128)\n",
    "        self.fc2 = nn.Linear(128, classes)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "        \n",
    "    \n",
    "    ## defines how the data moves across the layers\n",
    "    def forward(self,x):\n",
    "        forw = self.conv1(x)\n",
    "        forw = self.max_pool1(forw)\n",
    "        \n",
    "        forw = self.conv2(forw)\n",
    "        forw = self.max_pool2(forw)\n",
    "                \n",
    "        forw = forw.reshape(forw.size(0), -1)\n",
    "        \n",
    "        forw = self.fc1(forw)\n",
    "        forw = self.fc2(forw)\n",
    "        forw = self.softmax(forw)\n",
    "        \n",
    "\n",
    "        return forw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define the model from the class we built\n",
    "model = BespokeCNN(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, epochs, train_dl, valid_dl, learning_rate):\n",
    "    \"\"\"\n",
    "    This function will train the provided model printing the accuracy for the validation data.\n",
    "    It takes the model, number of epochs, and two data loaders and then runs the model through the \n",
    "    training process. If we want to change the loss function or optimizer, we can in the code below\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.001, momentum = 0.9)  \n",
    "    #total_step = len(train_dl)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        #Load in the data in batches using the train_loader object\n",
    "        for i, (images, labels) in enumerate(train_dl):  \n",
    "            # Move tensors to the configured device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        ## let's monitor the prediction accuracy on the validation data for each epoch\n",
    "        ## this may indicate a good stopping point for the particular neural network\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for images, labels in valid_dl:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "            print('Accuracy on {} validation images: {:.4f} %'.format(val_size, 100 * correct / total))\n",
    "\n",
    "        end = time.time()\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}, Time in Seconds: {:.4f}'.format(epoch+1, epochs, loss.item(), end - start))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 52.7253 %\n",
      "Epoch [1/25], Loss: 1.5926, Time in Seconds: 24.4088\n",
      "Accuracy on 2807 validation images: 52.7253 %\n",
      "Epoch [1/25], Loss: 1.5926, Time in Seconds: 24.4088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 54.9341 %\n",
      "Epoch [2/25], Loss: 1.5307, Time in Seconds: 23.9522\n",
      "Accuracy on 2807 validation images: 54.9341 %\n",
      "Epoch [2/25], Loss: 1.5307, Time in Seconds: 23.9522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 54.9341 %\n",
      "Epoch [3/25], Loss: 1.5617, Time in Seconds: 23.7382\n",
      "Accuracy on 2807 validation images: 54.9341 %\n",
      "Epoch [3/25], Loss: 1.5617, Time in Seconds: 23.7382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 55.8603 %\n",
      "Epoch [4/25], Loss: 1.4687, Time in Seconds: 23.8442\n",
      "Accuracy on 2807 validation images: 55.8603 %\n",
      "Epoch [4/25], Loss: 1.4687, Time in Seconds: 23.8442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 56.5372 %\n",
      "Epoch [5/25], Loss: 1.3823, Time in Seconds: 23.3743\n",
      "Accuracy on 2807 validation images: 56.5372 %\n",
      "Epoch [5/25], Loss: 1.3823, Time in Seconds: 23.3743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 56.1454 %\n",
      "Epoch [6/25], Loss: 1.3027, Time in Seconds: 23.8263\n",
      "Accuracy on 2807 validation images: 56.1454 %\n",
      "Epoch [6/25], Loss: 1.3027, Time in Seconds: 23.8263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 57.3922 %\n",
      "Epoch [7/25], Loss: 1.4306, Time in Seconds: 23.6338\n",
      "Accuracy on 2807 validation images: 57.3922 %\n",
      "Epoch [7/25], Loss: 1.4306, Time in Seconds: 23.6338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 59.1022 %\n",
      "Epoch [8/25], Loss: 1.5030, Time in Seconds: 23.8514\n",
      "Accuracy on 2807 validation images: 59.1022 %\n",
      "Epoch [8/25], Loss: 1.5030, Time in Seconds: 23.8514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 58.4966 %\n",
      "Epoch [9/25], Loss: 1.2865, Time in Seconds: 23.8525\n",
      "Accuracy on 2807 validation images: 58.4966 %\n",
      "Epoch [9/25], Loss: 1.2865, Time in Seconds: 23.8525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 58.3185 %\n",
      "Epoch [10/25], Loss: 1.4515, Time in Seconds: 23.5675\n",
      "Accuracy on 2807 validation images: 58.3185 %\n",
      "Epoch [10/25], Loss: 1.4515, Time in Seconds: 23.5675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 60.5629 %\n",
      "Epoch [11/25], Loss: 1.2663, Time in Seconds: 23.7524\n",
      "Accuracy on 2807 validation images: 60.5629 %\n",
      "Epoch [11/25], Loss: 1.2663, Time in Seconds: 23.7524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 59.6366 %\n",
      "Epoch [12/25], Loss: 1.4705, Time in Seconds: 23.5645\n",
      "Accuracy on 2807 validation images: 59.6366 %\n",
      "Epoch [12/25], Loss: 1.4705, Time in Seconds: 23.5645\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 60.7054 %\n",
      "Epoch [13/25], Loss: 1.4525, Time in Seconds: 23.9887\n",
      "Accuracy on 2807 validation images: 60.7054 %\n",
      "Epoch [13/25], Loss: 1.4525, Time in Seconds: 23.9887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 62.0235 %\n",
      "Epoch [14/25], Loss: 1.4016, Time in Seconds: 23.7018\n",
      "Accuracy on 2807 validation images: 62.0235 %\n",
      "Epoch [14/25], Loss: 1.4016, Time in Seconds: 23.7018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 60.4560 %\n",
      "Epoch [15/25], Loss: 1.4029, Time in Seconds: 23.4981\n",
      "Accuracy on 2807 validation images: 60.4560 %\n",
      "Epoch [15/25], Loss: 1.4029, Time in Seconds: 23.4981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 62.5579 %\n",
      "Epoch [16/25], Loss: 1.4704, Time in Seconds: 23.9585\n",
      "Accuracy on 2807 validation images: 62.5579 %\n",
      "Epoch [16/25], Loss: 1.4704, Time in Seconds: 23.9585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 63.2348 %\n",
      "Epoch [17/25], Loss: 1.3993, Time in Seconds: 23.6773\n",
      "Accuracy on 2807 validation images: 63.2348 %\n",
      "Epoch [17/25], Loss: 1.3993, Time in Seconds: 23.6773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 62.7360 %\n",
      "Epoch [18/25], Loss: 1.4288, Time in Seconds: 23.7531\n",
      "Accuracy on 2807 validation images: 62.7360 %\n",
      "Epoch [18/25], Loss: 1.4288, Time in Seconds: 23.7531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 64.8735 %\n",
      "Epoch [19/25], Loss: 1.3159, Time in Seconds: 23.6075\n",
      "Accuracy on 2807 validation images: 64.8735 %\n",
      "Epoch [19/25], Loss: 1.3159, Time in Seconds: 23.6075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 63.3060 %\n",
      "Epoch [20/25], Loss: 1.3714, Time in Seconds: 23.4025\n",
      "Accuracy on 2807 validation images: 63.3060 %\n",
      "Epoch [20/25], Loss: 1.3714, Time in Seconds: 23.4025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 64.0542 %\n",
      "Epoch [21/25], Loss: 1.4479, Time in Seconds: 24.8049\n",
      "Accuracy on 2807 validation images: 64.0542 %\n",
      "Epoch [21/25], Loss: 1.4479, Time in Seconds: 24.8049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 63.1635 %\n",
      "Epoch [22/25], Loss: 1.3894, Time in Seconds: 24.2381\n",
      "Accuracy on 2807 validation images: 63.1635 %\n",
      "Epoch [22/25], Loss: 1.3894, Time in Seconds: 24.2381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 64.7310 %\n",
      "Epoch [23/25], Loss: 1.4283, Time in Seconds: 23.6714\n",
      "Accuracy on 2807 validation images: 64.7310 %\n",
      "Epoch [23/25], Loss: 1.4283, Time in Seconds: 23.6714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 63.4129 %\n",
      "Epoch [24/25], Loss: 1.3788, Time in Seconds: 24.1553\n",
      "Accuracy on 2807 validation images: 63.4129 %\n",
      "Epoch [24/25], Loss: 1.3788, Time in Seconds: 24.1553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 65.3723 %\n",
      "Epoch [25/25], Loss: 1.3588, Time in Seconds: 24.7434\n",
      "Accuracy on 2807 validation images: 65.3723 %\n",
      "Epoch [25/25], Loss: 1.3588, Time in Seconds: 24.7434\n"
     ]
    }
   ],
   "source": [
    "modelOut = trainModel(model, 25, train_dl, valid_dl, learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 25 epochs the loss function is headed downwards and the prediction accuracy is increasing. This suggests more training cycles could be helpful. I'm going to re-run with 50 to see what I get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 42.7146 %\n",
      "Epoch [1/50], Loss: 1.6391, Time in Seconds: 25.3216\n",
      "Accuracy on 2807 validation images: 42.7146 %\n",
      "Epoch [1/50], Loss: 1.6391, Time in Seconds: 25.3216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 48.8422 %\n",
      "Epoch [2/50], Loss: 1.5251, Time in Seconds: 24.1999\n",
      "Accuracy on 2807 validation images: 48.8422 %\n",
      "Epoch [2/50], Loss: 1.5251, Time in Seconds: 24.1999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 52.6185 %\n",
      "Epoch [3/50], Loss: 1.3587, Time in Seconds: 23.9948\n",
      "Accuracy on 2807 validation images: 52.6185 %\n",
      "Epoch [3/50], Loss: 1.3587, Time in Seconds: 23.9948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 52.9035 %\n",
      "Epoch [4/50], Loss: 1.6053, Time in Seconds: 24.3778\n",
      "Accuracy on 2807 validation images: 52.9035 %\n",
      "Epoch [4/50], Loss: 1.6053, Time in Seconds: 24.3778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 54.0791 %\n",
      "Epoch [5/50], Loss: 1.5078, Time in Seconds: 23.8178\n",
      "Accuracy on 2807 validation images: 54.0791 %\n",
      "Epoch [5/50], Loss: 1.5078, Time in Seconds: 23.8178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 54.9341 %\n",
      "Epoch [6/50], Loss: 1.5962, Time in Seconds: 23.9399\n",
      "Accuracy on 2807 validation images: 54.9341 %\n",
      "Epoch [6/50], Loss: 1.5962, Time in Seconds: 23.9399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 54.6847 %\n",
      "Epoch [7/50], Loss: 1.5428, Time in Seconds: 23.8549\n",
      "Accuracy on 2807 validation images: 54.6847 %\n",
      "Epoch [7/50], Loss: 1.5428, Time in Seconds: 23.8549\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 56.6797 %\n",
      "Epoch [8/50], Loss: 1.5144, Time in Seconds: 23.5247\n",
      "Accuracy on 2807 validation images: 56.6797 %\n",
      "Epoch [8/50], Loss: 1.5144, Time in Seconds: 23.5247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 56.1454 %\n",
      "Epoch [9/50], Loss: 1.5776, Time in Seconds: 23.7885\n",
      "Accuracy on 2807 validation images: 56.1454 %\n",
      "Epoch [9/50], Loss: 1.5776, Time in Seconds: 23.7885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 56.7510 %\n",
      "Epoch [10/50], Loss: 1.3387, Time in Seconds: 24.4587\n",
      "Accuracy on 2807 validation images: 56.7510 %\n",
      "Epoch [10/50], Loss: 1.3387, Time in Seconds: 24.4587\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 57.8197 %\n",
      "Epoch [11/50], Loss: 1.3349, Time in Seconds: 24.1277\n",
      "Accuracy on 2807 validation images: 57.8197 %\n",
      "Epoch [11/50], Loss: 1.3349, Time in Seconds: 24.1277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 58.9597 %\n",
      "Epoch [12/50], Loss: 1.4548, Time in Seconds: 25.3639\n",
      "Accuracy on 2807 validation images: 58.9597 %\n",
      "Epoch [12/50], Loss: 1.4548, Time in Seconds: 25.3639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 59.5654 %\n",
      "Epoch [13/50], Loss: 1.3941, Time in Seconds: 24.7617\n",
      "Accuracy on 2807 validation images: 59.5654 %\n",
      "Epoch [13/50], Loss: 1.3941, Time in Seconds: 24.7617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 59.5297 %\n",
      "Epoch [14/50], Loss: 1.3212, Time in Seconds: 24.3005\n",
      "Accuracy on 2807 validation images: 59.5297 %\n",
      "Epoch [14/50], Loss: 1.3212, Time in Seconds: 24.3005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 60.7054 %\n",
      "Epoch [15/50], Loss: 1.5078, Time in Seconds: 24.0327\n",
      "Accuracy on 2807 validation images: 60.7054 %\n",
      "Epoch [15/50], Loss: 1.5078, Time in Seconds: 24.0327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 59.4941 %\n",
      "Epoch [16/50], Loss: 1.3955, Time in Seconds: 24.7815\n",
      "Accuracy on 2807 validation images: 59.4941 %\n",
      "Epoch [16/50], Loss: 1.3955, Time in Seconds: 24.7815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 58.7460 %\n",
      "Epoch [17/50], Loss: 1.5382, Time in Seconds: 23.9968\n",
      "Accuracy on 2807 validation images: 58.7460 %\n",
      "Epoch [17/50], Loss: 1.5382, Time in Seconds: 23.9968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 60.5273 %\n",
      "Epoch [18/50], Loss: 1.5624, Time in Seconds: 24.0338\n",
      "Accuracy on 2807 validation images: 60.5273 %\n",
      "Epoch [18/50], Loss: 1.5624, Time in Seconds: 24.0338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 60.9191 %\n",
      "Epoch [19/50], Loss: 1.5547, Time in Seconds: 24.2441\n",
      "Accuracy on 2807 validation images: 60.9191 %\n",
      "Epoch [19/50], Loss: 1.5547, Time in Seconds: 24.2441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 60.5985 %\n",
      "Epoch [20/50], Loss: 1.2432, Time in Seconds: 24.1416\n",
      "Accuracy on 2807 validation images: 60.5985 %\n",
      "Epoch [20/50], Loss: 1.2432, Time in Seconds: 24.1416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 62.2373 %\n",
      "Epoch [21/50], Loss: 1.4530, Time in Seconds: 23.7670\n",
      "Accuracy on 2807 validation images: 62.2373 %\n",
      "Epoch [21/50], Loss: 1.4530, Time in Seconds: 23.7670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 62.4154 %\n",
      "Epoch [22/50], Loss: 1.5444, Time in Seconds: 24.1739\n",
      "Accuracy on 2807 validation images: 62.4154 %\n",
      "Epoch [22/50], Loss: 1.5444, Time in Seconds: 24.1739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 62.8785 %\n",
      "Epoch [23/50], Loss: 1.3406, Time in Seconds: 24.3020\n",
      "Accuracy on 2807 validation images: 62.8785 %\n",
      "Epoch [23/50], Loss: 1.3406, Time in Seconds: 24.3020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 62.2373 %\n",
      "Epoch [24/50], Loss: 1.2806, Time in Seconds: 24.5073\n",
      "Accuracy on 2807 validation images: 62.2373 %\n",
      "Epoch [24/50], Loss: 1.2806, Time in Seconds: 24.5073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 63.3416 %\n",
      "Epoch [25/50], Loss: 1.4416, Time in Seconds: 25.1522\n",
      "Accuracy on 2807 validation images: 63.3416 %\n",
      "Epoch [25/50], Loss: 1.4416, Time in Seconds: 25.1522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 62.0948 %\n",
      "Epoch [26/50], Loss: 1.4307, Time in Seconds: 24.6753\n",
      "Accuracy on 2807 validation images: 62.0948 %\n",
      "Epoch [26/50], Loss: 1.4307, Time in Seconds: 24.6753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 64.5173 %\n",
      "Epoch [27/50], Loss: 1.5421, Time in Seconds: 23.6883\n",
      "Accuracy on 2807 validation images: 64.5173 %\n",
      "Epoch [27/50], Loss: 1.5421, Time in Seconds: 23.6883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 63.5910 %\n",
      "Epoch [28/50], Loss: 1.2988, Time in Seconds: 25.1394\n",
      "Accuracy on 2807 validation images: 63.5910 %\n",
      "Epoch [28/50], Loss: 1.2988, Time in Seconds: 25.1394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 64.4104 %\n",
      "Epoch [29/50], Loss: 1.2871, Time in Seconds: 23.8512\n",
      "Accuracy on 2807 validation images: 64.4104 %\n",
      "Epoch [29/50], Loss: 1.2871, Time in Seconds: 23.8512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 64.8379 %\n",
      "Epoch [30/50], Loss: 1.5197, Time in Seconds: 23.3971\n",
      "Accuracy on 2807 validation images: 64.8379 %\n",
      "Epoch [30/50], Loss: 1.5197, Time in Seconds: 23.3971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 63.8760 %\n",
      "Epoch [31/50], Loss: 1.3158, Time in Seconds: 24.2176\n",
      "Accuracy on 2807 validation images: 63.8760 %\n",
      "Epoch [31/50], Loss: 1.3158, Time in Seconds: 24.2176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 65.1942 %\n",
      "Epoch [32/50], Loss: 1.4810, Time in Seconds: 24.2327\n",
      "Accuracy on 2807 validation images: 65.1942 %\n",
      "Epoch [32/50], Loss: 1.4810, Time in Seconds: 24.2327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 64.1254 %\n",
      "Epoch [33/50], Loss: 1.3319, Time in Seconds: 24.2471\n",
      "Accuracy on 2807 validation images: 64.1254 %\n",
      "Epoch [33/50], Loss: 1.3319, Time in Seconds: 24.2471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 65.6929 %\n",
      "Epoch [34/50], Loss: 1.2282, Time in Seconds: 24.0863\n",
      "Accuracy on 2807 validation images: 65.6929 %\n",
      "Epoch [34/50], Loss: 1.2282, Time in Seconds: 24.0863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 65.2298 %\n",
      "Epoch [35/50], Loss: 1.2275, Time in Seconds: 23.8344\n",
      "Accuracy on 2807 validation images: 65.2298 %\n",
      "Epoch [35/50], Loss: 1.2275, Time in Seconds: 23.8344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 65.2654 %\n",
      "Epoch [36/50], Loss: 1.3097, Time in Seconds: 24.2290\n",
      "Accuracy on 2807 validation images: 65.2654 %\n",
      "Epoch [36/50], Loss: 1.3097, Time in Seconds: 24.2290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 65.3367 %\n",
      "Epoch [37/50], Loss: 1.2854, Time in Seconds: 24.1789\n",
      "Accuracy on 2807 validation images: 65.3367 %\n",
      "Epoch [37/50], Loss: 1.2854, Time in Seconds: 24.1789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 65.6217 %\n",
      "Epoch [38/50], Loss: 1.4903, Time in Seconds: 23.7771\n",
      "Accuracy on 2807 validation images: 65.6217 %\n",
      "Epoch [38/50], Loss: 1.4903, Time in Seconds: 23.7771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 65.4079 %\n",
      "Epoch [39/50], Loss: 1.5298, Time in Seconds: 23.7807\n",
      "Accuracy on 2807 validation images: 65.4079 %\n",
      "Epoch [39/50], Loss: 1.5298, Time in Seconds: 23.7807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 66.6548 %\n",
      "Epoch [40/50], Loss: 1.2338, Time in Seconds: 24.2001\n",
      "Accuracy on 2807 validation images: 66.6548 %\n",
      "Epoch [40/50], Loss: 1.2338, Time in Seconds: 24.2001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 67.1535 %\n",
      "Epoch [41/50], Loss: 1.4182, Time in Seconds: 23.9306\n",
      "Accuracy on 2807 validation images: 67.1535 %\n",
      "Epoch [41/50], Loss: 1.4182, Time in Seconds: 23.9306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 66.5123 %\n",
      "Epoch [42/50], Loss: 1.3421, Time in Seconds: 24.5953\n",
      "Accuracy on 2807 validation images: 66.5123 %\n",
      "Epoch [42/50], Loss: 1.3421, Time in Seconds: 24.5953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 67.6523 %\n",
      "Epoch [43/50], Loss: 1.3574, Time in Seconds: 23.7444\n",
      "Accuracy on 2807 validation images: 67.6523 %\n",
      "Epoch [43/50], Loss: 1.3574, Time in Seconds: 23.7444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 67.6167 %\n",
      "Epoch [44/50], Loss: 1.4547, Time in Seconds: 23.8040\n",
      "Accuracy on 2807 validation images: 67.6167 %\n",
      "Epoch [44/50], Loss: 1.4547, Time in Seconds: 23.8040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 66.7260 %\n",
      "Epoch [45/50], Loss: 1.4221, Time in Seconds: 23.6053\n",
      "Accuracy on 2807 validation images: 66.7260 %\n",
      "Epoch [45/50], Loss: 1.4221, Time in Seconds: 23.6053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 66.4410 %\n",
      "Epoch [46/50], Loss: 1.2321, Time in Seconds: 23.5884\n",
      "Accuracy on 2807 validation images: 66.4410 %\n",
      "Epoch [46/50], Loss: 1.2321, Time in Seconds: 23.5884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 66.6192 %\n",
      "Epoch [47/50], Loss: 1.2753, Time in Seconds: 23.7463\n",
      "Accuracy on 2807 validation images: 66.6192 %\n",
      "Epoch [47/50], Loss: 1.2753, Time in Seconds: 23.7463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 67.4385 %\n",
      "Epoch [48/50], Loss: 1.3611, Time in Seconds: 24.0111\n",
      "Accuracy on 2807 validation images: 67.4385 %\n",
      "Epoch [48/50], Loss: 1.3611, Time in Seconds: 24.0111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 65.7642 %\n",
      "Epoch [49/50], Loss: 1.3243, Time in Seconds: 23.8575\n",
      "Accuracy on 2807 validation images: 65.7642 %\n",
      "Epoch [49/50], Loss: 1.3243, Time in Seconds: 23.8575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on 2807 validation images: 67.2960 %\n",
      "Epoch [50/50], Loss: 1.4187, Time in Seconds: 24.0643\n",
      "Accuracy on 2807 validation images: 67.2960 %\n",
      "Epoch [50/50], Loss: 1.4187, Time in Seconds: 24.0643\n"
     ]
    }
   ],
   "source": [
    "modelOut = trainModel(model, 50, train_dl, valid_dl, learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to top out at 67% accuracy. Compared to other models, this is not very good. Models like ResNet9 have gotten a 92% accuracy on the same data. Reviewing why:\n",
    "\n",
    "- This is not deep. The famously strong models are deeper and use extra bells and whistles in the training process.\n",
    "- The model is simple. Two convolutional and pooling layers with two fully connected layers before a softmax activation function is probably not going to get the job done. It's hardly convolutional, and some might even say that it isn't.\n",
    "- Low model capacity. I intentionally set this low because I'm running this on my personal CPU, as such, a model with higher capacity will likely take eons to train due to my lack of a GPU\n",
    "\n",
    "#### Summary\n",
    "Ultimately, I would want to implement a well-known model or two and compare the performance. This exercise was meant to show my ability to create, implement, train and evaluate a bespoke neural network. Success! The model is poor, but it is a model built from scratch. If I were chasing performance I would go about this **very differently**. The reader would never see this bespoke model because I gravitate towards well-researched approaches. There is a lot of peer-review literature in this world, I am not smarter than the folks who do this research for their careers.\n",
    "\n",
    "#### Thank you for making it this far!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
